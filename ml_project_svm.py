# -*- coding: utf-8 -*-
"""ML_project_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WSicN6FVM4HXerA-oqa9p-lZQ4tCPMb8
"""

import numpy as np
import pandas as pd
# plotting
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# nltk
from nltk.stem import WordNetLemmatizer
# sklearn
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import accuracy_score

df = pd.read_csv('/content/drive/MyDrive/kaggle_parsed_dataset.csv')
df.shape

dff = df[['oh_label', 'Text']]

dff['oh_label'].nunique()

is_bully = dff[dff['oh_label']==1]
is_nobully = dff[dff['oh_label']==0]

print(len(is_bully))
print(len(is_nobully))
# uniqueWords = list(set(" ".join(r1).lower().split(" ")))
# count = len(uniqueWords)

N = 5
men_means = (2806)
women_means = (5993)

ind = 0.1
width = 0.35      
plt.bar(ind, men_means, width, label='Bully')
plt.bar(ind + width, women_means, width,
    label='Not bully')

plt.ylabel('Labels')
#plt.title('Scores by group and gender')

#plt.xticks(ind + width / 2, ('G1'))
plt.legend(loc='best')
plt.show()

dset = pd.concat([is_bully,is_nobully])
dset

dset['Text']=dset['Text'].str.lower()
dset

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

stopwordlst = stopwords.words()


def delete_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in stopwordlst])
dset['Text'] = dset['Text'].apply(lambda text: delete_stopwords(text))
dset['Text'].head()

import string

punctuation_list = string.punctuation

def cleaning_punctuations(text):
    translator = str.maketrans('', '', punctuation_list)
    return text.translate(translator)
dset['Text']= dset['Text'].apply(lambda x: cleaning_punctuations(x))
dset.head()

import re

def cleaning_repeating_char(text):
    return re.sub(r'(.)1+', r'1', text)
dset['Text'] = dset['Text'].apply(lambda x: cleaning_repeating_char(x))

dset.head()

def cleaning_URLs(data):
    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)
dset['Text'] = dset['Text'].apply(lambda x: cleaning_URLs(x))

dset.head()

def cleaning_numbers(data):
    return re.sub('[0-9]+', '', data)
dset['Text'] = dset['Text'].apply(lambda x: cleaning_numbers(x))
dset.head()

import nltk
st = nltk.PorterStemmer()
def stemming_on_text(data):
    text = [st.stem(word) for word in data]
    return data
dset['Text']= dset['Text'].apply(lambda x: stemming_on_text(x))
dset['Text'].head()

nltk.download('wordnet')
lm = nltk.WordNetLemmatizer()
def lemmatizer_on_text(data):
    text = [lm.lemmatize(word) for word in data]
    return data
dset['Text'] = dset['Text'].apply(lambda x: lemmatizer_on_text(x))
dset['Text'].head()

from collections import Counter
result = Counter(" ".join(dset['Text'].values.tolist()).split(" ")).items()
len(result)

data_neg = dset['Text'][2806:]
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
               collocations=False).generate(" ".join(data_neg))
plt.imshow(wc)

x = dset.Text
y = dset.oh_label

X_train, X_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state =26105111)

print(len(X_train))

max_words = list(set(" ".join(X_train).split()))
max_words_cnt = len(max_words)
max_len = X_train.apply(lambda x: len(x)).max()
max_words_cnt, max_len

vectorizer = TfidfVectorizer(
                             max_features = 150)
train_vectors = vectorizer.fit_transform(X_train)
test_vectors = vectorizer.transform(X_test)

def model_Evaluate(model):
  y_pred = model.predict(test_vectors)
  print(accuracy_score(y_test, y_pred))
  print('error ', ' ', 1-accuracy_score(y_test, y_pred))
  # Print the evaluation metrics for the dataset.
  print(classification_report(y_test, y_pred))
  # # Compute and plot the Confusion matrix
  cf_matrix = confusion_matrix(y_test, y_pred)
  categories = ['0','1']
  group_names = ['True Neg','False Pos', 'False Neg','True Pos']
  group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]
  labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',
  xticklabels = categories, yticklabels = categories)
  plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
  plt.ylabel("Actual values" , fontdict = {'size':14}, labelpad = 10)
  plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

def model_Evaluate1(model):
  y_pred = model.predict(train_vectors)
  print(accuracy_score(y_train, y_pred))
  print('error ', ' ', 1-accuracy_score(y_train, y_pred))
  # Print the evaluation metrics for the dataset.
  print(classification_report(y_train, y_pred))
  # # Compute and plot the Confusion matrix
  cf_matrix = confusion_matrix(y_train, y_pred)
  categories = ['0','1']
  group_names = ['True Neg','False Pos', 'False Neg','True Pos']
  group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]
  labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',
  xticklabels = categories, yticklabels = categories)
  plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
  plt.ylabel("Actual values" , fontdict = {'size':14}, labelpad = 10)
  plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

from sklearn import svm
from sklearn.metrics import classification_report
# Perform classification with SVM, kernel=linear
svm_model = svm.SVC(kernel='linear')
svm_model.fit(train_vectors, y_train)

model_Evaluate(svm_model) #test

model_Evaluate1(svm_model) #train

xxlst = [150, 1000, 10000, 15000, 23501]
ylst = [0.2679, 0.1958, 0.0910, 0.0785, 0.0607]
ylst1 = [0.2545, 0.1737, 0.1867, 0.1978, 0.1981]
y11 = np.arange(len(xxlst))
plt.plot(xxlst, ylst,color='blue', marker='.', label = 'training error')
plt.plot(xxlst, ylst1,color='red', marker='.', label = 'test error')
plt.xticks( xxlst)
plt.xlabel('number of features')
plt.ylabel('error')
plt.legend(loc="best")
plt.show()

from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score

XX = vectorizer.fit_transform(x)

loo = LeaveOneOut()

#calculate cross validated (leave one out) accuracy score
scores = cross_val_score(svm_model, XX,y, cv = 10, scoring='accuracy')

print( scores.mean() )

"""**Till this point I did lematizing vecotrizing error measure and CV measure. Then found out there's overfitting. So, now tried to change kernal and C for regularization 
** bold text
"""

LRmodel = LogisticRegression(C = 0.5, max_iter = 500, n_jobs=-1)
LRmodel.fit(train_vectors, y_train)

LRmodel1 = LogisticRegression(C = 10, max_iter = 500, n_jobs=-1)
LRmodel1.fit(train_vectors, y_train)

model_Evaluate(LRmodel) #test accuracy LR

model_Evaluate1(LRmodel) #train accuracy LR

model_Evaluate1(LRmodel1) #train accuracy LR

model_Evaluate(LRmodel1) #test accuracy LR

wLR = LRmodel.coef_
#print(len(wLR[0]))

wLR1 = LRmodel1.coef_
#print(len(wLR1[0]))
wlst = []
for i in range(0,200):
  wlst.append(wLR[0][i])
wlst1= []
for i in range(0,200):
  wlst1.append(wLR1[0][i])

xLst=[]
cnt=1
for i in range(200):
  xLst.append(cnt)
  cnt+=1

plt.plot(xLst, wlst,color='blue', marker='.', label='C=0.5')
plt.plot(xLst, wlst1,color='red', marker='.', label='C=10')
plt.legend(loc="best")
plt.show()

wLR1 = LRmodel1.coef_
#print(len(wLR1[0]))
wlst = []
for i in range(1,201):
  wlst.append(wLR1[0][i])

print(len(wlst))

xLst=[]
cnt=1
for i in range(200):
  xLst.append(cnt)
  cnt+=1

plt.plot(xLst, wlst,color='yellow', marker='.')
#plt.plot(xLst, wLR1[0],color='red', marker='v')
plt.show()

from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}

grid = GridSearchCV(svm.SVC(),param_grid,refit=True,verbose=2)
grid.fit(train_vectors,y_train)

print(grid.best_estimator_)

#print(grid)
new_svm_model = svm.SVC(kernel = 'linear', gamma = 1, C= 0.5)
new_svm_model.fit(train_vectors, y_train)

model_Evaluate1(new_svm_model) #train accuracy

model_Evaluate(new_svm_model) #test accuracy

print(new_svm_model.support_vectors_)

w1 = svm_model.coef_
print(w1)

print(vectorizer.vocabulary_)
print(len(X_train))

w3 = LRmodel.coef_
print(len(w3[0]))

from sklearn.neighbors import KNeighborsClassifier

# KNN_model = KNeighborsClassifier(n_neighbors=3)
# KNN_model.fit(train_vectors, y_train)

error_rate = []
# Might take some time
for i in range(1,85):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(train_vectors, y_train)
    pred_i = knn.predict(test_vectors)
    error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,85),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

KNN_model = KNeighborsClassifier(n_neighbors=23)
KNN_model.fit(train_vectors, y_train)

model_Evaluate1(KNN_model) #train accuracy knn

model_Evaluate(KNN_model) #test accuracy knn